{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from hyperdash import monitor_cell\n",
    "\n",
    "import pickle\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "TWEET_LEN = 140\n",
    "MAX_LEN = 200\n",
    "#VOC_SIZE = 50000\n",
    "#EMBEDDING_DIM = 100\n",
    "\n",
    "HIDDEN_DIM = 1024\n",
    "DEPTH = 3\n",
    "FIRST_DROPOUT = 0.3\n",
    "LATER_DROPOUT = 0.5\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 8\n",
    "\n",
    "# Other Constants\n",
    "DATASET_FILE = \"trump_tweets.pickle\"\n",
    "MODEL_FILE = \"Models/TrumpTweetRegularized({}-{}).h5\".format(DEPTH, HIDDEN_DIM)\n",
    "\n",
    "np.random.seed = 42\n",
    "char_list = list(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 ()#@,.:;-$?!/'\\\"\\n\")\n",
    "num2char = dict(enumerate(char_list, 1))\n",
    "num2char[0] = \"<PAD>\"\n",
    "char2num = dict(zip(num2char.values(), num2char.keys()))\n",
    "VOCAB_SIZE = len(char_list) + 1\n",
    "\n",
    "n_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle loaded: 31227 Tweets\n"
     ]
    }
   ],
   "source": [
    "tweets = pickle.load(open(DATASET_FILE, \"rb\"))\n",
    "print(\"Pickle loaded:\", len(tweets), \"Tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reshape Dataset like a boss!\n",
    "# Convers string to char/token arrays\n",
    "X = list(list(char2num[c] for c in t if c in char2num) for t in tweets)\n",
    "# Pad tweets to 140 chars\n",
    "X = np.array(list(tweet + (TWEET_LEN - len(tweet))*[0] for tweet in X if len(tweet) < TWEET_LEN))\n",
    "# Get onehot-encodings\n",
    "X = np.eye(VOCAB_SIZE)[X]\n",
    "# Create X -> y pairs\n",
    "y = X[:, 1:, :]\n",
    "X = X[:, :-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper Methods\n",
    "def generate_text(length):\n",
    "    ix = [np.random.randint(VOCAB_SIZE)]\n",
    "    y_char = [num2char[ix[-1]]]\n",
    "    X = np.zeros((1, length, VOCAB_SIZE))\n",
    "    for i in range(length):\n",
    "        X[0, i, ix[-1]] = 1\n",
    "        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
    "        if ix[-1] != 0:\n",
    "            y_char.append(num2char[ix[-1]])\n",
    "    return ('').join(y_char)\n",
    "\n",
    "def generate_tweets(num_tweets):\n",
    "    ix = np.zeros((num_tweets, 1, VOCAB_SIZE))\n",
    "    for a in ix:\n",
    "        a[0, np.random.randint(VOCAB_SIZE)] = 1\n",
    "    while True:\n",
    "        iy = model.predict(ix)[:, ix.shape[1]-1, :]\n",
    "        c = np.array([np.random.choice(np.arange(VOCAB_SIZE), p=ps) for ps in iy])\n",
    "        #c = np.array([c = np.argmax(ps)) for ps in iy])    \n",
    "        #c = np.argmax(iy[0, 0])\n",
    "        if np.all(c==0) or ix.shape[1] >= MAX_LEN:\n",
    "            break\n",
    "        nx = np.eye(VOCAB_SIZE)[c].reshape(num_tweets, 1, VOCAB_SIZE)\n",
    "        ix = np.concatenate((ix, nx), axis=1)\n",
    "    tweets = [\"\".join(num2char[n] for n in np.argmax(tweet, axis=1) if n != 0) for tweet in ix]\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, None, 1024)        4526080   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, None, 1024)        8392704   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, None, 1024)        8392704   \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, None, 80)          82000     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, None, 80)          0         \n",
      "=================================================================\n",
      "Total params: 21,393,488\n",
      "Trainable params: 21,393,488\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = Sequential()\n",
    "model.add(layers.LSTM(HIDDEN_DIM, input_shape=(None, VOCAB_SIZE), return_sequences=True, dropout=FIRST_DROPOUT))\n",
    "for i in range(DEPTH - 1):\n",
    "    model.add(layers.LSTM(HIDDEN_DIM, return_sequences=True, dropout=LATER_DROPOUT))\n",
    "model.add(layers.TimeDistributed(layers.Dense(VOCAB_SIZE)))\n",
    "model.add(layers.Activation('softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored model\n"
     ]
    }
   ],
   "source": [
    "# Optional: Restore model from checkpoint\n",
    "model = keras.models.load_model(\"models/goodTrump(3-1024).h5\")\n",
    "print(\"Restored model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commencing training\n",
      "Train on 26255 samples, validate on 2918 samples\n",
      "Epoch 1/1\n",
      "26255/26255 [==============================] - 656s - loss: 0.6875 - acc: 0.7984 - val_loss: 0.9687 - val_acc: 0.7368\n",
      "60 stories over the world to see the failing @nytimes is so instincts about me in the polls - they ar\n",
      "Completed 1. epoch\n",
      "________________________________________________________________________________________________________________________\n",
      "Train on 26255 samples, validate on 2918 samples\n",
      "Epoch 1/1\n",
      " 6048/26255 [=====>........................] - ETA: 478s - loss: 0.6600 - acc: 0.8058"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-083b3ae09a2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Commencing training\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mn_epoch\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMODEL_FILE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mF:\\Programme\\Anaconda\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    861\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32mF:\\Programme\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mF:\\Programme\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mF:\\Programme\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2269\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mF:\\Programme\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mF:\\Programme\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mF:\\Programme\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mF:\\Programme\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mF:\\Programme\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Min Valid Loss = 0.967 512-0.3-0-0\n",
    "print(\"Commencing training\")\n",
    "while True:\n",
    "    hist = model.fit(X, y, validation_split=0.1, batch_size=BATCH_SIZE, epochs=1)\n",
    "    n_epoch += 1\n",
    "    model.save(MODEL_FILE, overwrite=True)\n",
    "    print(generate_text(100))\n",
    "    print(\"Completed {}. epoch\".format(n_epoch))\n",
    "    print(\"_\"*120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(generate_text(140))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$5.9This mogger is very support the menswear best your intast success.', '1 thee people finally have the right things - this Aran problem still have no product everyone. Why can richal happiness, but they are no longer situations!', 'First of Dasahan Mocker is a total fool-basch--which on top of Christmas pretty who released $6,440 people to TPP uslimited ', 'Will be thinking of @Macys -- Egg!  They dont we are fighting to listen to DC for Christmas! I will stop you stop!', 'Obama did not have a personal horrendoused after 9 DAME  AMERICAN GOOST BORES WILL CHINGE PLANITY STANTEDS illegal immigration!', 'Amazing vaccinations last night, why are they now say 36.6 AM ISTALICE FOR TRUMP!', 'small john. Massive case by modern suited nite guys that is arresady to mention. Stay a much time he would looser bow. ', 'via @nypost by @JoeNBC: Trump says Judgthe 2 Norman rooms lives @MikePenaeytD! ???? ', \"n@Scottish11  if We don't get played great on thete are, get a real cause for \", '@jvhitewillishope  Great @drewlare  Thanks.', 'final resolts are wrong down Obama to succeed, WWI RNVE STRIPAD- HUT WAF ON THE POLLS! #N', \"chuck State Apprentice All-Star's failing @TrumpCollection. Together, we were the Lexs 5  \", 'people are really special and haddened complicated Forumbed of the Top - against 6.8,000 people watched them or nice!', 'located in Pergunanisti B, Dallas in NF for a landmark of family.  ', 'It is easyen in the Senate, now Irish Carl Inside this  @FoxNews', 'ha6 Penn Open? ', '85 effort at 10:30am from a father- beauty courage. Look forward to making me terrific hearther!', 'Keep fighting About @mariamenounom in France and Dio. Will #AmericaFirst #Trump2016 ', 'Via @wypen by @ericneyjo: @realDonaldTrump needs to be punished, only one has a personal view of life now.  Watch ', 'run  training week to do so we can make America great again. Can we really read.', 'aid as \" Democrats purchases the president many hacks near their eleven\" and for one to it!  Watch: ', '50 stories better having the sweer or Atlantic City supporter ensrg ruleing public ahead of impeccable. This time he cannot all be fine dirty tyro-late.', \"You may watch lows of radical Islamic and visit this overrated, and getting what's right.  Steve Jobs and Watch!\", 'Flashback: Everything come on participate lines for @PiersMorgan is in such a great hotel about storing in history', \"via @NewsmaxMedia by @dashenzington: @Debate Would's Bird Ceremony: \\nKevin Washington Poli: \", 'David Letterman need down by $155 more years. Set the phony Takeare Mike.U Said, the party of your ship without the election!', 'Japan gives treate  you release the Pathetic losing e-mails. You can do a third rally last night!', 'Larginis coach Khyzick Poll: Minding Press for next free a star of the Birthdays  ', \"They should have a dumbest thing that's why, Let Trump then Republicans learn you a long time here in a long time ago.\", 'ul polls spend like a holiday gift. Never retract 100 to 11 unless resolate carrier out. Be released.  Think Like a Champion', ' If Obaras new poll immediately did not get into White House, in little staff and she will go to News?', \"'Just leaving Sec. Berified and National Sprillion FLORIDA, a lifetime Times- \", \"Xndering StephenBallower get red crazy!\\nLet's take a lousy candidate, to discuss the clip for Fiscal Clinton.\", ';@keystotto  attacked 3 million @sethmeys  is his electric coming to mo! No surprise!', 'n and Douglas Chuckaees can watch his first debate. He uses energy respected but bad y political suts. Big an election!', 'Flashback: \"Filming Trump Invitational candidacy Twitter spotsbacktheart effort partson-just stated I\\'m with Bill Maher.\"', 'zeek w/ Governor Cuomo calls a new book \"Obama is fulling up and ours!\" Always Dubioutijizy for him to lead!', 'Questions was ruining Hillary Clinton and her husband grates. NHANK  SAD!', 'dMR-@BrwtPaul In Sumrel Fathers-Voter Ruo delivers #AmericaFirst? ', 'from heading to Georgia for another 5 million jown but have made so bad. An audience when said I have a death size!', '50 stories above the past, I have ever seen, our strong borders say that he has lied to help it loss.    Dale Carnegie', 'deed works previawe themselves\" Watch this stuff! He is expanding on me. This is the best way to keep responsible for yourses.', 'zhen Re: Rev. Obama has the standed for the Trump Tower. And funly cheat is already delivered.', '20 Mitt Romney says  suit was a soldier of doing me the my statement on heart, the stupid will be fully turned out out  determony with us!', '\\nDumor of Justice Double Governor of Bernie Senserol influenced 350 yrs for debt getting away from expanding the accurate in strength, speaking   ', 'Luge on Benghazi. ', 'Just like Rubio. Ke is a terrific newspaper--we love you and I must fil the new prospective. Thanks!', 'quick choice Tax Bush, must focus in the right Big Only is on his serve this will be forgotten-looking for a lot.', \"Personally, I mean represible time level's joint Romney, the Old Post Office builder the future by an using a fine.\", 'people believe that Mexico borry is over! #TrumpTower ', \"Little @Macy's, it is time to Quis. This is making me as approved. @usopengolf is one of the worst I have ever fine direct how to definitely go!\", ')@Dallasnews because Mr Trump really shows that 10 years of 10 Miality Action... ', 'join me in Scotland course to Iran  Louisiana \\n#DrainTheSwamp #Debates2016 ', 'stupid condition coming, election LOVED machines over praise to last election to succeed, because of your friends all lies.', 'Its sad, I like, but if they are looking good in political full headquarters. Get your momentum - minute directly there.', 'crobe the same editor @GovernorPurion one day in Podishew, who spent her total fucation!', 'girlfriends espected,njust wasted on fantastic strengtheI now \"Feuding By OUR investment in Deffalt\"', '', '(@CJNManno had a massive rally fundraiser, it will be disaster. So true!  @ApprenticeNBC seeks too lucky to USS!', \"90 stories over $60M in upwellectly's veterans, looks like a larger screenjent in the World. \", '80 stories $1B for the Republicans and Kates did not have experience and was prepared no longer doing badly. Thats (good luck.', \"jrdy in our of our country we're exactly what was! Walt it to targets!\", '$1,000,000 Presidential 120,000 Quick Convention ', 'Join me tomorrow at 4:00pm! #ImWithYou\\nVideo:  ', 'jfpeaking new Potentian relationship to make a foreug winner from Ciss West. A great honor going will blow the debt.', 'die Coming for something more important for ridious deals in Lubry. Watch: ', '#TBT Full Trump National Golf Course ', '-- @MittRomney off of 15 King (Soarc), @Squawksen. Call me soon at his Sunday March 3rd.', 'in the inaugural GOP games, @TrumpChicago for building up parolina desperately staffers the first Old Post Office on Medicor', 'problems are taken off to the V.. Republicans on the people of hia was yesterday, it will never be the same win.\"', 'Stupid policies promoted in Miami empane to give another great tonight. It was a swandrituen.', \"given chemo puiling Trump National Doral in Miami. Nice's in at 4 in Scotland w/ @SanthuNEVilve!\", 'President Obama to continue our brave million-dollars docline, she has told him down  he has disgraced during the Obama use the Virginia.  Make America Great Again!', 'Entrepreneurs: As a total warring one.', \"You can't let this the wall of families @Disney Johnstown for a fight to the planet--do that?\", 'Emmys target @DannyZuker in favorite the @titchingthig2, a good shot.', 'Hia @BreitbartNews by GOP Hander @TrumpGolfLA this past @Autismspeaks is \"Doing the Windfills\"  ', \"New Government just can't watch T- MS 13th season of All-Star  @ApprenticeNBC - just received great resort!\", 'elegate that you support the Pope, of THE MESS USE. Good work is that being found out.', 'Entrepreneurili: UKTimetic Entrepreneurs:\\nLots of reality cheap lack. Hopefully ,  it is the Class - better we all claim that work next.', 'I guess @billmaher has falling access to all very debate--why jost Governor Richer, which is going into Obama.', '.@HillaryCallson interview lost yet the new ! #StayTravelerd  via @Zathyaluo', '60 story was sowewalled until this Obama CAUST when she works are not when Globad scorcial statewesting against them. This country is sad!', ',Wow, the Republicins can now be Virginia. See story, I know it. We need strong and smart energy, youll have thinking - a very good question!', '99 story @TrumpTowerNY will be the only parent of location w/GOP - tour office. Perfect straight!', 'JOBS, JOBIS! ', ':(@johnmar21 @AmSpec artisless  Megate Republican cantilate Trump lictures scrupped by Geraldo will be speaking in 28\"', 'pay-for all the dram when it talks to repeal --- @AWTULKE said \"Carson with him!\\'Boois\" is not exactly what\\'s marking it comes', ':ZR,anders? Success is not so politician you\\'re for someone to vote for.\" -- Jackwelch ', 'Thank you Florida. America needs to make a difference. Written by Waller is terrific!', '34 days until Mark Cuban offers who yo or defending Obamacare, @TrumpGolfLA features the wonderful wreat warriors best ', ' Theres no questions. There are overyoo a nightmome dogether. Think Big and whatever her.', \"drue Marty on the Dems are so leaph. Let's Make America Great Again! \", 'positive endorsementment coming into the late-of you go to be as you renovate--see ut to be you to bring for your environments. - Think Big', 'I will be interviewed on Fox and Friends in two champion. Her Pin Lex Anmenical. Thank you Michigan!', 'our new transport. Obama is extated, with ruring jobs in America. Beggar giving!', 'XECHACHOWAR: Mininary Principation is going to change on his campaign, Obama to put a quote!', \":I am the only man who can start @AMSpec at the best response! You must run out of killer. I'll say it too, what the hell, you're too soliditian.\", '45 (getting over new book, and losses--not done at your people in the Policy. Congratulations on your Obama and WH', 'the Joker out of National Security Advisor Unchinien Repeal   Syria that REPEAL of its election! What a crowd. Ofam countries soon']\n"
     ]
    }
   ],
   "source": [
    "print(generate_tweets(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
